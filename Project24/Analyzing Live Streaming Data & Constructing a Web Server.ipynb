{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b420fa95",
   "metadata": {},
   "source": [
    "# Analyzing Live Streaming Data & Constructing a Web Server\n",
    "\n",
    "**Mauricio Ferragut**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e9cef0",
   "metadata": {},
   "source": [
    "# Index\n",
    "- [Abstract](#-Abstract)\n",
    "- [Part 1: Streaming Live Data to ThingsBoard](#Part-1:-Streaming-Live-Data-to-ThingsBoard)\n",
    "- [Part 2: Analyzing Live Data Using ThingsBoard](#Part-2:-Analyzing-Live-Data-Using-ThingsBoard)\n",
    "- [Part 3: Constructing a Web Server Using Kafka](#Part-3:-Constructing-a-Web-Server-Using-Kafka)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76bef44",
   "metadata": {},
   "source": [
    "[Back to top](#Index)\n",
    "## Abstract\n",
    "The first part of this project demonstrates the streaming live data from IoT devices (in this case a thermometer and hygrometer), to ThingsBoard where it is processed by a rule chain and posted to a realtime database in Firebase. \n",
    "\n",
    "The second part of this project demonstrates analysis of live streamed data by populating a separate field in the realtime database when the temperature exceeds parameters. \n",
    "\n",
    "In the third and final part of this project, I set up a python application to publish vehicle location data to a Kafka topic. Kafka is implemented as a Docker container (image created by Confluent) and sends messages to a web server created with Node.js where they are consumed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4b0d01",
   "metadata": {},
   "source": [
    "[Back to top](#Index)\n",
    "## Part 1: Streaming Live Data to ThingsBoard\n",
    "\n",
    "The first part of this project sets up the Mosquitto, ThingsBoard, and Firebase environments. Mosquitto and ThingsBoard are set up as Docker containers, and a Realtime database is initialized in Firebase. \n",
    "\n",
    "The MQTT (Message Queuing Telemetry Transport) protocol is set up to produce temperature and humidity data as if there were a real thermometer and hygrometer producing and transmitting information every 3 seconds. Temperature values are generated as random integers between 0 and 100, and humidity values are generated as random integers between 50 and 100. The code used can be found in 'Project 24\\Project_24_MQTT\\ThingsBoard\\paho-mqtt\\TBpublish.py'.\n",
    "\n",
    "Next, the data produced by the MQTT protocol is published to ThingsBoard. \n",
    "\n",
    "Finally, a ThingsBoard rule chain is created to send the temperature and humidity data is sent to the Realtime database.\n",
    "\n",
    "Detailed steps and screenshots can be found in the PDF document for Part 1 in the Github repository for this project.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9068c7e",
   "metadata": {},
   "source": [
    "[Back to top](#Index)\n",
    "## Part 2: Analyzing Live Data Using ThingsBoard\n",
    "\n",
    "The second part of this project sets up an alarm rule chain to send information about live streaming data that is above a certain threshold to Firebase. Any event in which the temperature exceeds 80 degrees will trigger the alarm rule chain, which sends the temperature and humidity values for that event to a the alarm field rather than the normal temperature field in the realtime database. This is a simple example of realtime data analysis, which would allow further action to be taken given exceeded parameters. For example, if we had a thermostat connected to the system as well, we would be able to trigger the thermostat to turn on and cool the building down to a determined temperature. This would all be automated through ThingsBoard and would not require a person to be involved at any step of the process.\n",
    "\n",
    "As with part 1, the detailed steps and screenshots of the process can be found in the PDF for Part 2 in the Github repository for this project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82a373b",
   "metadata": {},
   "source": [
    "[Back to top](#Index)\n",
    "## Part 3: Constructing a Web Server Using Kafka\n",
    "\n",
    "In this final part of the project, I worked with Kafka, one of the most successful applications for handling the streaming of big data at scale. I used a Docker image created by Confluent that installs all of the necessary Kafka components including, among others, the broker and ZooKeeper. I began with a simple Kafka implementation that I used to create a Python application that publishes vehicle location longitude-latitude data to a Kafka topic. Next, I used Node.js to start a web server that acts as a consumer for the messages received from the Kafka application."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
